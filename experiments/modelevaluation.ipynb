{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855039b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90241305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\jhasu\\\\Desktop\\\\Mlops_template\\\\experiments'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94918ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c828627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\jhasu\\\\Desktop\\\\Mlops_template'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ebd0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class modelevaluationentity:\n",
    "    model_path: Path\n",
    "    test_data_path: Path\n",
    "    image_size: tuple\n",
    "    batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c2c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_name.Utills.utills import read_yaml\n",
    "from project_name.contants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "\n",
    "\n",
    "class configManager:\n",
    "    def __init__(self,config = CONFIG_FILE_PATH,params = PARAMS_FILE_PATH):\n",
    "        self.config =  read_yaml(config)\n",
    "        self.params = read_yaml(params)\n",
    "        os.makedirs(self.config.artifact_root,exist_ok=True)\n",
    "    \n",
    "    def get_evaluation_entity(self):\n",
    "        config= self.config.model_evaluation\n",
    "        entity = modelevaluationentity(\n",
    "            model_path= config.model_dir,\n",
    "            test_data_path= config.data_dir,\n",
    "            image_size= self.params.image_size,\n",
    "            batch_size= self.params.batch_size\n",
    "        )\n",
    "        return entity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d82cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import mlflow\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, entity:modelevaluationentity):\n",
    "        self.entity = entity\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Load model\n",
    "        model = torch.load(self.entity.model_path, map_location=self.device)\n",
    "        model.eval()\n",
    "\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(self.entity.image_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        test_dataset = datasets.ImageFolder(self.entity.test_data_path, transform=transform)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.entity.batch_size)\n",
    "\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(y_true, y_pred, target_names=test_dataset.classes, output_dict=True)\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_dict(report, \"classification_report.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
